"""Create user and other tables

Revision ID: a574fc167cb9
Revises: f9fe4f99f1c5
Create Date: 2025-08-25 02:22:43.235567

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy.schema import UniqueConstraint # Import UniqueConstraint if needed for future proofing
from sqlalchemy import text # Import text for postgresql_where

# revision identifiers, used by Alembic.
revision = 'a574fc167cb9'
down_revision = 'f9fe4f99f1c5'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('user',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('username', sa.String(length=80), nullable=False),
    sa.Column('password', sa.String(length=128), nullable=False),
    sa.Column('role', sa.String(length=50), nullable=True),
    sa.Column('business_id', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['business_id'], ['businesses.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('username')
    )
    op.drop_table('users')
    op.drop_table('categories')
    with op.batch_alter_table('businesses', schema=None) as batch_op:
        batch_op.alter_column('last_synced_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               type_=sa.DateTime(),
               existing_nullable=True)
        batch_op.create_unique_constraint(None, ['remote_id'])

    with op.batch_alter_table('company_transactions', schema=None) as batch_op:
        batch_op.add_column(sa.Column('description', sa.Text(), nullable=True))
        batch_op.drop_constraint(batch_op.f('company_transactions_company_id_fkey'), type_='foreignkey')
        batch_op.drop_column('company_id')
        batch_op.drop_column('notes')

    with op.batch_alter_table('inventory_items', schema=None) as batch_op:
        # IMPORTANT: First, drop the incorrect simple barcode index if it was actually created
        # You're already dropping uq_inventory_item_business_barcode, which is good.
        # This line was trying to create the problematic index:
        batch_op.drop_index(batch_op.f('uq_inventory_item_business_barcode'), postgresql_where='(barcode IS NOT NULL)') # This line is already good

        # CORRECTED LINE: Create a composite unique index on (business_id, barcode)
        batch_op.create_index(
            'idx_unique_active_barcode', # Name of the index
            ['business_id', 'barcode'],  # Columns for the composite index
            unique=True,                 # Ensure uniqueness
            postgresql_where=text('barcode IS NOT NULL') # Apply condition if barcode is nullable
        )

    with op.batch_alter_table('sales_records', schema=None) as batch_op:
        batch_op.alter_column('synced_to_remote',
               existing_type=sa.BOOLEAN(),
               nullable=False)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('sales_records', schema=None) as batch_op:
        batch_op.alter_column('synced_to_remote',
               existing_type=sa.BOOLEAN(),
               nullable=True)

    with op.batch_alter_table('inventory_items', schema=None) as batch_op:
        # Correctly drop the composite index in downgrade
        batch_op.drop_index('idx_unique_active_barcode', postgresql_where=sa.text('barcode IS NOT NULL'))
        # Recreate the old, potentially incorrect index or a simpler one if needed for downgrade
        # This part depends on what the previous migration (f9fe4f99f1c5) created or intended.
        # For simplicity, if the previous one was just 'uq_inventory_item_business_barcode' without WHERE clause,
        # you might recreate that here, or re-run 'flask db migrate' after downgrade if you intend to revert completely.
        # Given the error was on CREATE UNIQUE INDEX, you likely didn't have this one existing,
        # so simply dropping the new one is enough for downgrade.
        
        # If the original uq_inventory_item_business_barcode was also a problem,
        # you might need to drop that one too, and then if needed, re-create it without unique if it was intended to be non-unique.
        # For now, let's assume the goal is just to revert the composite unique index.
        # op.create_index(batch_op.f('uq_inventory_item_business_barcode'), ['business_id', 'barcode'], unique=True, postgresql_where='(barcode IS NOT NULL)')
        # Re-adding the original faulty index for downgrade, but it's okay because we are pushing to upgrade
        batch_op.create_index(batch_op.f('uq_inventory_item_business_barcode'), ['business_id', 'barcode'], unique=True, postgresql_where=sa.text('(barcode IS NOT NULL)'))


    with op.batch_alter_table('company_transactions', schema=None) as batch_op:
        batch_op.add_column(sa.Column('notes', sa.TEXT(), autoincrement=False, nullable=True))
        batch_op.add_column(sa.Column('company_id', sa.VARCHAR(length=36), autoincrement=False, nullable=False))
        batch_op.create_foreign_key(batch_op.f('company_transactions_company_id_fkey'), 'companies', ['company_id'], ['id'])
        batch_op.drop_column('description')

    with op.batch_alter_table('businesses', schema=None) as batch_op:
        batch_op.drop_constraint(None, type_='unique') # This might drop the remote_id constraint, handle carefully
        batch_op.alter_column('last_synced_at',
               existing_type=sa.DateTime(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=True)

    op.create_table('categories',
    sa.Column('id', sa.VARCHAR(length=36), autoincrement=False, nullable=False),
    sa.Column('business_id', sa.VARCHAR(length=36), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(length=100), autoincrement=False, nullable=False),
    sa.Column('is_active', sa.BOOLEAN(), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['business_id'], ['businesses.id'], name=op.f('categories_business_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('categories_pkey')),
    sa.UniqueConstraint('name', 'business_id', name=op.f('_category_name_business_uc'))
    )
    op.create_table('users',
    sa.Column('id', sa.VARCHAR(length=36), autoincrement=False, nullable=False),
    sa.Column('username', sa.VARCHAR(length=100), autoincrement=False, nullable=False),
    sa.Column('role', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('business_id', sa.VARCHAR(length=36), autoincrement=False, nullable=False),
    sa.Column('password_hash', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('is_active', sa.BOOLEAN(), server_default=sa.text('true'), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['business_id'], ['businesses.id'], name=op.f('users_business_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('users_pkey')),
    sa.UniqueConstraint('username', 'business_id', name=op.f('_username_business_uc'))
    )
    op.drop_table('user')
    # ### end Alembic commands ###